{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/seglass5/PartIIIProject/blob/master/Part_III_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQe8GUC-hjT9"
   },
   "source": [
    "# 1. Install Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 327571,
     "status": "ok",
     "timestamp": 1716633467234,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "TA97pKdXqnPV",
    "outputId": "ecf758d0-66b3-4b1d-a59d-8e9727888f19"
   },
   "outputs": [],
   "source": [
    "# Python packages\n",
    "!pip install tensorboardX\n",
    "!pip install pot\n",
    "!pip install -U networkx\n",
    "!pip install matplotlib==3.1.1\n",
    "!pip install torchvision\n",
    "\n",
    "# Custom packages from github to allow RandWire to work, and FLOPs counter to work\n",
    "!pip install git+https://github.com/JiaminRen/CVdevKit.git\n",
    "!pip install git+git://github.com/sovrasov/flops-counter.pytorch.git@64d38fd47cb0795437056745c64a987d944c1885\n",
    "!pip install igraph\n",
    "!pip install ptflops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJwjXcYuhpwy"
   },
   "source": [
    "# 2. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7345,
     "status": "ok",
     "timestamp": 1716633543714,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "-Q5lWaKGrXo0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from ptflops import get_model_complexity_info\n",
    "import tensorboardX\n",
    "import yaml\n",
    "import CVdevKit\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import ot\n",
    "import importlib\n",
    "import numpy as np\n",
    "import gym\n",
    "import sys\n",
    "import csv\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json\n",
    "from google.colab import files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1716633549241,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "GbOBd35MYEvg"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBsAJEl2hx8a"
   },
   "source": [
    "# 3. Mount Drive and Import Packages and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27144,
     "status": "ok",
     "timestamp": 1716633720966,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "dBsDfiJT0mEf",
    "outputId": "9310fb4f-20e5-4eda-dd60-06b0bc7609df"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6Yb5-3uiA_u"
   },
   "source": [
    "# 4. Train model of Randomly Wired Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1716633725888,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "0xxFWvySg0w3"
   },
   "outputs": [],
   "source": [
    "def remove_isolated_nodes(graph):\n",
    "  isolated_nodes = []\n",
    "  for node in list(graph.nodes()):\n",
    "    if(graph.degree[node] == 0):\n",
    "      isolated_nodes.append(node)\n",
    " # print(isolated_nodes)\n",
    "  graph.remove_nodes_from(isolated_nodes)\n",
    "  return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1716633726709,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "rd5OEltFw5bH"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Node = collections.namedtuple('Node', ['id', 'inputs', 'type'])\n",
    "\n",
    "def get_graph_info(graph):\n",
    "\n",
    "  input_nodes = []\n",
    "  output_nodes = []\n",
    "  Nodes = []\n",
    "  for node in range(graph.number_of_nodes()):\n",
    "    tmp = list(graph.neighbors(node))\n",
    "    tmp.sort()\n",
    "\n",
    "    type = -1\n",
    "    if node < tmp[0]:\n",
    "      input_nodes.append(node)\n",
    "      type = 0\n",
    "    if node > tmp[-1]:\n",
    "      output_nodes.append(node)\n",
    "      type = 1\n",
    "    Nodes.append(Node(node, [n for n in tmp if n < node], type))\n",
    "\n",
    "  return Nodes, input_nodes, output_nodes\n",
    "\n",
    "def build_graph(Nodes, cfg ):\n",
    "  if cfg.GRAPH_MODEL == 'ER':\n",
    "    return nx.random_graphs.erdos_renyi_graph(Nodes, cfg.ER_P, cfg.RND_SEED)\n",
    "  elif cfg.GRAPH_MODEL == 'BA':\n",
    "    return nx.random_graphs.barabasi_albert_graph(Nodes, cfg.BA_M,cfg.RND_SEED)\n",
    "  elif cfg.GRAPH_MODEL == 'WS':\n",
    "    return nx.random_graphs.connected_watts_strogatz_graph(Nodes, cfg.WS_K, cfg.WS_P, tries=200, seed=cfg.RND_SEED)\n",
    "\n",
    "def save_graph(graph, path):\n",
    "  nx.write_yaml(graph, path)\n",
    "\n",
    "def save_graphml(graph,path):\n",
    "  nx.write_graphml(graph,path)\n",
    "\n",
    "def load_graph(path):\n",
    "  return nx.read_yaml(path)\n",
    "\n",
    "def load_graphml(path):\n",
    "  return nx.read_graphml(path, node_type = int).copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 845,
     "status": "ok",
     "timestamp": 1716633730612,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "vpX_fy3uxOhi"
   },
   "outputs": [],
   "source": [
    "# referred to JiaminRen's implementation\n",
    "# https://github.com/JiaminRen/RandWireNN\n",
    "\n",
    "class conv_unit(nn.Module):\n",
    "    def __init__(self, nin, nout, stride):\n",
    "        super(conv_unit, self).__init__()\n",
    "        # print(\"conv_unit : nin : {}, nout: {}, stride :{}\".format(nin,nout, stride))\n",
    "        self.depthwise_separable_conv_3x3 = nn.Conv2d(nin, nin, kernel_size=3, stride=stride, padding=1, groups=nin)\n",
    "        self.pointwise_conv_1x1 = nn.Conv2d(nin, nout, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise_separable_conv_3x3(x)\n",
    "        out = self.pointwise_conv_1x1(out)\n",
    "        return out\n",
    "\n",
    "class Triplet_unit(nn.Module):\n",
    "    def __init__(self, inplanes, outplanes, stride=1):\n",
    "        super(Triplet_unit, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv = conv_unit(inplanes, outplanes, stride)\n",
    "        self.bn = nn.BatchNorm2d(outplanes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(x)\n",
    "        out = self.conv(out)\n",
    "        out = self.bn(out)\n",
    "        return out\n",
    "\n",
    "class Node_OP(nn.Module):\n",
    "    def __init__(self, Node, inplanes, outplanes):\n",
    "        super(Node_OP, self).__init__()\n",
    "        self.is_input_node = Node.type == 0\n",
    "        self.input_nums = len(Node.inputs)\n",
    "        if self.input_nums > 1:\n",
    "            self.mean_weight = nn.Parameter(torch.ones(self.input_nums))\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "        if self.is_input_node:\n",
    "            self.conv = Triplet_unit(inplanes, outplanes, stride=2)\n",
    "        else:\n",
    "            self.conv = Triplet_unit(outplanes, outplanes, stride=1)\n",
    "\n",
    "    def forward(self, *input):\n",
    "        if self.input_nums > 1:\n",
    "            out = self.sigmoid(self.mean_weight[0]) * input[0]\n",
    "        for i in range(1, self.input_nums):\n",
    "            out = out + self.sigmoid(self.mean_weight[i]) * input[i]\n",
    "        else:\n",
    "            out = input[0]\n",
    "            out = self.conv(out)\n",
    "        return out\n",
    "\n",
    "class StageBlock(nn.Module):\n",
    "    def __init__(self, graph, inplanes, outplanes):\n",
    "        super(StageBlock, self).__init__()\n",
    "        self.nodes, self.input_nodes, self.output_nodes = get_graph_info(graph)\n",
    "        self.nodeop  = nn.ModuleList()\n",
    "        for node in self.nodes:\n",
    "            self.nodeop.append(Node_OP(node, inplanes, outplanes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        results = {}\n",
    "        for id in self.input_nodes:\n",
    "            results[id] = self.nodeop[id](x)\n",
    "        for id, node in enumerate(self.nodes):\n",
    "            if id not in self.input_nodes:\n",
    "                results[id] = self.nodeop[id](*[results[_id] for _id in node.inputs])\n",
    "        result = results[self.output_nodes[0]]\n",
    "        for idx, id in enumerate(self.output_nodes):\n",
    "            if idx > 0:\n",
    "                result = result + results[id]\n",
    "        result = result / len(self.output_nodes)\n",
    "        return result\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, cfg,measure):\n",
    "        super(Net, self).__init__()\n",
    "        # for image color scale\n",
    "        #color = cfg.NN.COLOR\n",
    "        color=3\n",
    "        #print(color)\n",
    "        N = cfg.NN.NODES\n",
    "        size = cfg.NN.IMG_SIZE\n",
    "        num_classes = cfg.NN.NUM_CLASSES\n",
    "\n",
    "\n",
    "        if (cfg[\"USE_PRUNED_GRAPH\"] == True):\n",
    "            %cd /content/drive/MyDrive/PRUNING/PRUNED/WS/\n",
    "            global pval,kval,seed\n",
    "            if (cfg['NN']['REGIME'] == \"SMALL\"):\n",
    "              graph2 = load_graphml('./WS_K_{}/WS_P_{}/seed_{}/SMALL/output_bt/{}/conv2.graphml'.format(kval,pval,seed,measure))\n",
    "              graph3 = load_graphml('./WS_K_{}/WS_P_{}/seed_{}/SMALL/output_bt/{}/conv3.graphml'.format(kval,pval,seed,measure))\n",
    "              graph4 = load_graphml('./WS_K_{}/WS_P_{}/seed_{}/SMALL/output_bt/{}/conv4.graphml'.format(kval,pval,seed,measure))\n",
    "              graph5 = load_graphml('./WS_K_{}/WS_P_{}/seed_{}/SMALL/output_bt/{}/conv5.graphml'.format(kval,pval,seed,measure))\n",
    "            if (cfg['NN']['REGIME'] == \"REGULAR\"):\n",
    "              graph2 = load_graphml('./WS_K_{}/WS_P_{}/seed_{}/REGULAR/output_bt/{}/conv2.graphml'.format(kval,pval,seed,measure))\n",
    "              graph3 = load_graphml('./WS_K_{}/WS_P_{}/seed_{}/REGULAR/output_bt/{}/conv3.graphml'.format(kval,pval,seed,measure))\n",
    "              graph4 = load_graphml('./WS_K_{}/WS_P_{}/seed_{}/REGULAR/output_bt/{}/conv4.graphml'.format(kval,pval,seed,measure))\n",
    "              graph5 = load_graphml('./WS_K_{}/WS_P_{}/seed_{}/REGULAR/output_bt/{}/conv5.graphml'.format(kval,pval,seed,measure))\n",
    "\n",
    "        elif cfg.MAKE_GRAPH:\n",
    "\n",
    "            %cd /content/drive/MyDrive/PRUNING/UNPRUNED/WS\n",
    "\n",
    "            graph2 = nx.convert_node_labels_to_integers(remove_isolated_nodes(build_graph(N//2, cfg)))\n",
    "            graph3 = nx.convert_node_labels_to_integers(remove_isolated_nodes(build_graph(N, cfg)))\n",
    "            graph4 = nx.convert_node_labels_to_integers(remove_isolated_nodes(build_graph(N, cfg)))\n",
    "            graph5 = nx.convert_node_labels_to_integers(remove_isolated_nodes(build_graph(N, cfg)))\n",
    "\n",
    "            global net_stats\n",
    "            net_stats['conv2_nodes']=graph2.number_of_nodes()\n",
    "            net_stats['conv2_edges']=graph2.number_of_edges()\n",
    "            net_stats['conv3_nodes']=graph3.number_of_nodes()\n",
    "            net_stats['conv3_edges']=graph3.number_of_edges()\n",
    "\n",
    "            if (cfg['NN']['REGIME'] == \"SMALL\"):\n",
    "              save_graphml(graph2, './WS_K_{}/WS_P_{}/seed_{}/SMALL/conv2_{}.graphml'.format(kval,pval,seed,cfg['RND_SEED']))\n",
    "              save_graphml(graph3, './WS_K_{}/WS_P_{}/seed_{}/SMALL/conv3_{}.graphml'.format(kval,pval,seed,cfg['RND_SEED']))\n",
    "              save_graphml(graph4, './WS_K_{}/WS_P_{}/seed_{}/SMALL/conv4_{}.graphml'.format(kval,pval,seed,cfg['RND_SEED']))\n",
    "              save_graphml(graph5, './WS_K_{}/WS_P_{}/seed_{}/SMALL/conv5_{}.graphml'.format(kval,pval,seed,cfg['RND_SEED']))\n",
    "            if (cfg['NN']['REGIME'] == \"REGULAR\"):\n",
    "              save_graphml(graph2, './WS_K_{}/WS_P_{}/seed_{}/REGULAR/conv2_{}.graphml'.format(kval,pval,seed,cfg['RND_SEED']))\n",
    "              save_graphml(graph3, './WS_K_{}/WS_P_{}/seed_{}/REGULAR/conv3_{}.graphml'.format(kval,pval,seed,cfg['RND_SEED']))\n",
    "              save_graphml(graph4, './WS_K_{}/WS_P_{}/seed_{}/REGULAR/conv4_{}.graphml'.format(kval,pval,seed,cfg['RND_SEED']))\n",
    "              save_graphml(graph5, './WS_K_{}/WS_P_{}/seed_{}/REGULAR/conv5_{}.graphml'.format(kval,pval,seed,cfg['RND_SEED']))\n",
    "        else:\n",
    "            graph2 = load_graph('./output/graph/conv2.yaml')\n",
    "            graph3 = load_graph('./output/graph/conv3.yaml')\n",
    "            graph4 = load_graph('./output/graph/conv4.yaml')\n",
    "            graph5 = load_graph('./output/graph/conv5.yaml')\n",
    "\n",
    "        if cfg.NN.REGIME == \"SMALL\":\n",
    "            print('small regime')\n",
    "            C = 78\n",
    "            self.conv1 =  nn.Sequential(\n",
    "                conv_unit(color, C//2, 2),\n",
    "                nn.BatchNorm2d(C//2)\n",
    "                )\n",
    "            self.conv2 = Triplet_unit(C//2, C)\n",
    "            self.conv3 = StageBlock(graph3, C, C)\n",
    "            self.conv4 = StageBlock(graph4, C, 2*C)\n",
    "            self.conv5 = StageBlock(graph5, 2*C, 4*C)\n",
    "            self.classifier = nn.Sequential(\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Conv2d(4*C, 1280, kernel_size=1),\n",
    "                    nn.BatchNorm2d(1280),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.AvgPool2d(size//16, stride=1),\n",
    "                )\n",
    "            self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "        if cfg.NN.REGIME == \"REGULAR\":\n",
    "            print('regular regime')\n",
    "            C = 109\n",
    "            self.conv1 =  nn.Sequential(\n",
    "                conv_unit(color, C//2, 2),\n",
    "                nn.BatchNorm2d(C//2)\n",
    "                )\n",
    "            self.conv2 = StageBlock(graph2, C//2,  C)\n",
    "            self.conv3 = StageBlock(graph3, C,   2*C)\n",
    "            self.conv4 = StageBlock(graph4, 2*C, 4*C)\n",
    "            self.conv5 = StageBlock(graph5, 4*C, 8*C)\n",
    "            self.classifier = nn.Sequential(\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.Conv2d(8*C, 1280, kernel_size=1),\n",
    "                    nn.BatchNorm2d(1280),\n",
    "                    nn.ReLU(True),\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.AvgPool2d(size//32, stride=1),\n",
    "                )\n",
    "            self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv4(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv5(x)\n",
    "        # print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        # print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 519,
     "status": "ok",
     "timestamp": 1716633736332,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "x3JvRqdYslaI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/Codes/')\n",
    "import torch\n",
    "from utils.config_helpers import merge_configs\n",
    "import time\n",
    "import networkx as nx\n",
    "\n",
    "def get_configuration():\n",
    "    # load configs for base network and data set\n",
    "    from RandWireNN_config import cfg as network_cfg\n",
    "    from utils.configs.ImageNet_config import cfg as dataset_cfg\n",
    "    return merge_configs([network_cfg, dataset_cfg])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1716633736332,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "D9XhWfkLYZ3j"
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    torch.manual_seed(1)\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.normal_(m.weight.data,mean = 0, std = 1/16)\n",
    "        nn.init.constant_(m.bias.data,0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.normal_(m.weight.data,mean=0, std=1/16)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight.data, mean = 0, std = 1/16)\n",
    "        nn.init.constant_(m.bias.data,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1716633738791,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "hmdc9AYro6e-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os, sys\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch, cfg):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(len(train_loader), batch_time, data_time, losses, top1,\n",
    "                             top5, prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    i=0\n",
    "\n",
    "    for input,target in train_loader:\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input = input.to(cfg.DEVICE)\n",
    "        target = target.to(cfg.DEVICE)\n",
    "\n",
    "        # compute output\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 2))\n",
    "        #acc1 ,acc= accuracy(output, target, topk=(1,1))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(acc1[0], input.size(0))\n",
    "        top5.update(acc5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % cfg.PRINT_FREQ == 0:\n",
    "            progress.print(i)\n",
    "            if cfg.VISDOM:\n",
    "                cfg.vis.line(X=torch.Tensor([epoch+(i/len(train_loader))]).unsqueeze(0).cpu(),\n",
    "                              Y=torch.Tensor([loss]).unsqueeze(0).cpu(),\n",
    "                              env='torch',win=cfg.loss_window,name='train_loss',update='append')\n",
    "        i+=1\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, cfg):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(len(val_loader), batch_time, losses, top1, top5,\n",
    "                             prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        i=0\n",
    "        for input,target in val_loader:\n",
    "\n",
    "          input = input.to(cfg.DEVICE)\n",
    "          target = target.to(cfg.DEVICE)\n",
    "\n",
    "          # compute output\n",
    "          output = model(input)\n",
    "          loss = criterion(output, target)\n",
    "\n",
    "          #print(output,\" \",target)\n",
    "          # measure accuracy and record loss\n",
    "          acc1, acc5 = accuracy(output, target, topk=(1, 2))\n",
    "          losses.update(loss.item(), input.size(0))\n",
    "          top1.update(acc1[0], input.size(0))\n",
    "          top5.update(acc5[0], input.size(0))\n",
    "\n",
    "          # measure elapsed time\n",
    "          batch_time.update(time.time() - end)\n",
    "          end = time.time()\n",
    "\n",
    "          if i % cfg.PRINT_FREQ == 0:\n",
    "              progress.print(i)\n",
    "\n",
    "      # TODO: this should also be done with the ProgressMeter\n",
    "\n",
    "          i+=1\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
    "\n",
    "    return losses.avg, top1.avg\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, *meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def print(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.reshape(1,-1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0,keepdim = True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "def prepare(cfg, use_arg_parser=True):\n",
    "    if not os.path.isdir(\"./output/\"):\n",
    "        os.mkdir(\"./output/\")\n",
    "    if not os.path.isdir(\"./output/model\"):\n",
    "        os.mkdir(\"./output/model\")\n",
    "    if not os.path.isdir(\"./output/graph\"):\n",
    "        os.mkdir(\"./output/graph\")\n",
    "    if not cfg.TEST_MODE:\n",
    "        if cfg.VISDOM:\n",
    "            cfg.loss_window = cfg.vis.line(\n",
    "                        Y=torch.zeros((1)).cpu(),\n",
    "                        X=torch.zeros((1)).cpu(),env='torch',\n",
    "                        opts=dict(xlabel='epoch',ylabel='Loss',\n",
    "                                    title=cfg.DATASET_NAME+\"_\"\n",
    "                                    +time.strftime(\"%m/%d %H:%M\", time.localtime()),\n",
    "                        legend=['train_loss','val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 818,
     "status": "ok",
     "timestamp": 1716633747901,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "CdAp-YeVIKEl"
   },
   "outputs": [],
   "source": [
    "def performance_measure(val_loader, model,bottom_val,top_val,measure,cfg):\n",
    "  from sklearn.metrics import confusion_matrix\n",
    "  import pandas as pd\n",
    "  global seed, pval,kval\n",
    "  nb_classes = 2\n",
    "  if(cfg['USE_PRUNED_GRAPH']):\n",
    "      %cd /content/drive/MyDrive/PRUNING/PRUNED/WS\n",
    "    # Train\n",
    "  else:\n",
    "    %cd /content/drive/MyDrive/PRUNING/UNPRUNED/WS\n",
    "\n",
    "  # Initialize the prediction and label lists(tensors)\n",
    "\n",
    "\n",
    "  if (not os.path.isdir(\"./WS_K_{}/WS_P_{}/seed_{}/{}/{}/\".format(kval,pval,num,cfg['NN']['REGIME'],measure))):\n",
    "        os.makedirs(\"./WS_K_{}/WS_P_{}/seed_{}/{}/{}/\".format(kval,pval,num,cfg['NN']['REGIME'],measure))\n",
    "\n",
    "  count = 0\n",
    "\n",
    "  Thresh_list=[[0.9,0.1]]\n",
    "  dic={}\n",
    "  cf={}\n",
    "  for k in Thresh_list:\n",
    "    dic[k[1]]={}\n",
    "    if (not os.path.isdir(\"./WS_K_{}/WS_P_{}/seed_{}/{}/{}/Thresh_{}/\".format(kval,pval,num,cfg['NN']['REGIME'],measure,k[1]))):\n",
    "        os.makedirs(\"./WS_K_{}/WS_P_{}/seed_{}/{}/{}/Thresh_{}/\".format(kval,pval,num,cfg['NN']['REGIME'],measure,k[1]))\n",
    "\n",
    "\n",
    "  for k in Thresh_list:\n",
    "    dic[k[1]]['TN']=0\n",
    "    dic[k[1]]['TP']=0\n",
    "    dic[k[1]]['FN']=0\n",
    "    dic[k[1]]['FP']=0\n",
    "    cf[k[1]]=[]\n",
    "  with torch.no_grad():\n",
    "\n",
    "    prediction_list=[]\n",
    "    actual_class_list=[]\n",
    "    one_count=0\n",
    "    zero_count=0\n",
    "    y_train=[]\n",
    "    y_train.clear()\n",
    "\n",
    "    pred=[]\n",
    "    pred.clear()\n",
    "\n",
    "    for inputs, classes in val_loader:\n",
    "\n",
    "        inputs = inputs.to(cfg.DEVICE)\n",
    "        classes = classes.to(cfg.DEVICE)\n",
    "\n",
    "        for i,x in enumerate(classes):\n",
    "\n",
    "          if x.item()==1:\n",
    "            one_count+=1\n",
    "          else:\n",
    "            zero_count+=1\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        probability = torch.nn.functional.softmax(outputs, 1)\n",
    "        probabilities=probability.to(cfg.DEVICE)\n",
    "        threshold = torch.tensor(k).unsqueeze(0)\n",
    "        thresholds=threshold.to(cfg.DEVICE)\n",
    "\n",
    "        predictions = probabilities >= thresholds\n",
    "        actual_class_list.clear()\n",
    "\n",
    "        for i,x in enumerate(classes):\n",
    "          actual_class_list.append(x.item())\n",
    "          y_train.append(x.item())\n",
    "\n",
    "        temp_c=0\n",
    "        for k in Thresh_list:\n",
    "          #print(\"Length\",len(prediction_list),\" \",len(actual_class_list))\n",
    "          prediction_list.clear()\n",
    "          access_tensor=0\n",
    "          for i,x in enumerate(probabilities):\n",
    "            for t in x:\n",
    "              if round(t.item(),2) >k[0]:\n",
    "                prediction_list.append(0)\n",
    "                cf[k[1]].append(0)\n",
    "              else:\n",
    "                prediction_list.append(1)\n",
    "                cf[k[1]].append(1)\n",
    "              break\n",
    "\n",
    "            for t in x:\n",
    "              if k[0]==0.9:\n",
    "                if access_tensor==0:\n",
    "                  access_tensor+=1\n",
    "                  continue\n",
    "                else:\n",
    "                  pred.append(round(t.item(),2))\n",
    "                  access_tensor=0\n",
    "\n",
    "         # print(len(prediction_list),\" \",len(actual_class_list))\n",
    "          for i in range(len(prediction_list)):\n",
    "            #print(i)\n",
    "            if prediction_list[i] ==0 and actual_class_list[i]==0:\n",
    "              dic[k[1]]['TN']+=1\n",
    "            if prediction_list[i] ==0 and actual_class_list[i]==1:\n",
    "              dic[k[1]]['FN']+=1\n",
    "            if prediction_list[i] ==1 and actual_class_list[i]==0:\n",
    "              dic[k[1]]['FP']+=1\n",
    "            if prediction_list[i] ==1 and actual_class_list[i]==1:\n",
    "              dic[k[1]]['TP']+=1\n",
    "          #else:\n",
    "            # Nothing=0\n",
    "  #print(dic)\n",
    "  for k in Thresh_list:\n",
    "\n",
    "    dic[k[1]]['TPR']=dic[k[1]]['TP']/(dic[k[1]]['TP']+dic[k[1]]['FN'])\n",
    "    dic[k[1]]['TNR']=dic[k[1]]['TN']/(dic[k[1]]['TN']+dic[k[1]]['FP'])\n",
    "    dic[k[1]]['FPR']=dic[k[1]]['FP']/(dic[k[1]]['TN']+dic[k[1]]['FP'])\n",
    "    dic[k[1]]['FNR']=dic[k[1]]['FN']/(dic[k[1]]['TP']+dic[k[1]]['FN'])\n",
    "    dic[k[1]]['Accuracy']=(dic[k[1]]['TP']+dic[k[1]]['TN'])/(dic[k[1]]['TP']+dic[k[1]]['FN']+dic[k[1]]['TN']+dic[k[1]]['FP'])\n",
    "    dic[k[1]]['Precision']=dic[k[1]]['TP']/(dic[k[1]]['TP']+dic[k[1]]['FP'])\n",
    "    dic[k[1]]['Recal/Sensitivity']=dic[k[1]]['TP']/(dic[k[1]]['TP']+dic[k[1]]['FN'])\n",
    "    dic[k[1]]['Specifity']=dic[k[1]]['TN']/(dic[k[1]]['TN']+dic[k[1]]['FP'])\n",
    "    dic[k[1]]['F1-Score']=(2*dic[k[1]]['Precision']*dic[k[1]]['Recal/Sensitivity'])/(dic[k[1]]['Precision']+dic[k[1]]['Recal/Sensitivity'])\n",
    "\n",
    "\n",
    "\n",
    "  print(dic)\n",
    "  print(\"Zero_Count\",zero_count)\n",
    "  print(\"One_Count\",one_count)\n",
    "  print(len(y_train),\" \",len(pred))\n",
    "  labels=['Threshold', 'TN', 'TP', 'FN', 'FP', 'TPR', 'TNR', 'FPR', 'FNR', 'Accuracy', 'Precision', 'Recall/Sensitivity', 'Specificity', 'F1-Score']\n",
    "\n",
    "  df=pd.DataFrame(dic)\n",
    "  data=df.swapaxes(\"rows\", \"columns\").reset_index()\n",
    "  data.columns=labels\n",
    "  data['Seed']=pd.Series([seed]*data.shape[0])\n",
    "  data['Graph-Model']=pd.Series(['WS']*data.shape[0])\n",
    "  data['K-value']=pd.Series([kval]*data.shape[0])\n",
    "  data['P-value']=pd.Series([pval]*data.shape[0])\n",
    "\n",
    "  data.to_csv(\"./WS_K_{}/WS_P_{}/seed_{}/{}/{}/PERFORMANCE_MEASURE_X_{}_Y_{}.csv\".format(kval,pval,num,cfg['NN']['REGIME'],measure,bottom_val,top_val), encoding='utf-8', sep = '\\t', header= True, index = False)\n",
    "\n",
    "\n",
    "  import numpy as np\n",
    "  from sklearn import metrics\n",
    "\n",
    "  import matplotlib\n",
    "  import matplotlib.pyplot as plt\n",
    "  from matplotlib.ticker import MaxNLocator\n",
    "  from sklearn.metrics import confusion_matrix\n",
    "  import seaborn as sns\n",
    "\n",
    "  matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "  matplotlib.rcParams['ps.fonttype'] = 42\n",
    "  matplotlib.rcParams['font.family'] = \"arial\"\n",
    "  plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "  plt.rcParams[\"figure.figsize\"] = (3,3)\n",
    "\n",
    "  for k in Thresh_list:\n",
    "    Confusion_matrix=confusion_matrix(y_train, cf[k[1]])\n",
    "    pd.Series(y_train).to_csv(\"./WS_K_{}/WS_P_{}/seed_{}/{}/{}/Thresh_{}/Confusion_matrix_act_val_X_{}_Y_{}.csv\".format(kval,pval,num,cfg['NN']['REGIME'],measure,k[1],bottom_val,top_val),index=False)\n",
    "    pd.Series(cf[k[1]]).to_csv(\"./WS_K_{}/WS_P_{}/seed_{}/{}/{}/Thresh_{}/Confusion_matrix_pred_val_X_{}_Y_{}.csv\".format(kval,pval,num,cfg['NN']['REGIME'],measure,k[1],bottom_val,top_val),index=False)\n",
    "\n",
    "\n",
    "    sns.heatmap(Confusion_matrix, annot=True, annot_kws={\"size\": 15},fmt=\"d\",cbar=False, linewidths=.5, cmap=\"Blues\",xticklabels=['Non Covid', 'Covid-19'], yticklabels=['Non Covid', 'Covid-19']) # font size\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(\"./WS_K_{}/WS_P_{}/seed_{}/{}/{}/Thresh_{}/CONFUSION_MATRIX_X_{}_Y_{}.pdf\".format(kval,pval,num,cfg['NN']['REGIME'],measure,k[1],bottom_val,top_val))\n",
    "    plt.show()\n",
    "    #print('\\n')\n",
    "\n",
    "  plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "  plt.rcParams.update({'font.size': 15})\n",
    "  plt.rcParams[\"axes.grid\"] = False\n",
    "  plt.rcParams['axes.facecolor']='white'\n",
    "  plt.rcParams['savefig.facecolor']='white'\n",
    "  plt.rcParams['figure.facecolor'] = 'white'\n",
    "  matplotlib.rc('axes',edgecolor='k')\n",
    "\n",
    "  ax=plt.figure().gca()\n",
    "  ax.spines['bottom'].set_linewidth(0.5)\n",
    "  ax.spines['left'].set_linewidth(0.5)\n",
    "  ax.tick_params(direction='out', length=0.5, width=2)\n",
    "\n",
    "  y = np.array(y_train)\n",
    "  prd = np.array(pred)\n",
    "  fpr, tpr, thresholds = metrics.roc_curve(y, prd)\n",
    "  pd.DataFrame(tpr).to_csv(\"./WS_K_{}/WS_P_{}/seed_{}/{}/{}/tpr_X_{}_Y_{}.csv\".format(kval,pval,num,cfg['NN']['REGIME'],measure,bottom_val,top_val),index=False)\n",
    "  pd.DataFrame(fpr).to_csv(\"./WS_K_{}/WS_P_{}/seed_{}/{}/{}/fpr_X_{}_Y_{}.csv\".format(kval,pval,num,cfg['NN']['REGIME'],measure,bottom_val,top_val),index=False)\n",
    "  RC=metrics.auc(fpr, tpr)\n",
    "  #print(\"RC\",RC)\n",
    "  plt.title('Receiver Operating Characteristic')\n",
    "  plt.plot(fpr, tpr,color='red',label='AUC = %0.4f'% RC)\n",
    "  plt.legend(loc='lower right')\n",
    "#  plt.plot([0,0],[0,1],'r--')\n",
    "  plt.xlim([-0.001, 1.01])\n",
    "  plt.ylim([-0.01, 1.01])\n",
    "  plt.ylabel('True Positive Rate')\n",
    "  plt.xlabel('False Positive Rate')\n",
    "  plt.savefig(\"./WS_K_{}/WS_P_{}/seed_{}/{}/{}/ROC_X_{}_Y_{}.pdf\".format(kval,pval,num,cfg['NN']['REGIME'],measure,bottom_val,top_val))\n",
    "  plt.show()\n",
    "\n",
    "  precision, recall, thresholds =metrics.precision_recall_curve(y, prd)\n",
    "  #plt.plot([0, 1],linestyle='--')\n",
    "\n",
    "  pd.DataFrame(precision).to_csv(\"./WS_K_{}/WS_P_{}/seed_{}/{}/{}/precision_X_{}_Y_{}.csv\".format(kval,pval,num,cfg['NN']['REGIME'],measure,bottom_val,top_val),index=False)\n",
    "  pd.DataFrame(recall).to_csv(\"./WS_K_{}/WS_P_{}/seed_{}/{}/{}/recall_X_{}_Y_{}.csv\".format(kval,pval,num,cfg['NN']['REGIME'],measure,bottom_val,top_val),index=False)\n",
    "\n",
    "  from sklearn.metrics import average_precision_score\n",
    "  average_precision = average_precision_score(y, prd)\n",
    "  plt.title('Precision Recall Curve')\n",
    "  plt.plot(recall, precision,color='green',label= 'Average Precision = %0.2f'% average_precision)\n",
    "  plt.legend(loc='lower left')\n",
    "  #plt.plot([0,0],[1,0],'r--')\n",
    "  plt.xlim([-0.001, 1.01])\n",
    "  plt.ylim([-0.01, 1.01])\n",
    "  plt.ylabel('Precision')\n",
    "  plt.xlabel('Recall')\n",
    "  plt.savefig(\"./WS_K_{}/WS_P_{}/seed_{}/{}/{}/PR_CURVE_X_{}_Y_{}.pdf\".format(kval,pval,num,cfg['NN']['REGIME'],measure,bottom_val,top_val))\n",
    "  plt.show()\n",
    "  return dic[0.1]['Accuracy'],dic[0.1]['Recal/Sensitivity'],dic[0.1]['Specifity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 443,
     "status": "ok",
     "timestamp": 1716633754276,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "27qaHAbj5AOr"
   },
   "outputs": [],
   "source": [
    "class set_environment():\n",
    "  def __init__(self):\n",
    "    self.acc_threshold = 0.8518\n",
    "  def reset(self,seed):\n",
    "    # Function to reset graphs to unpruned state\n",
    "    %cd /content/drive/MyDrive/output/graph/\n",
    "    conv_2_unpruned = nx.read_graphml('conv2_{}.graphml'.format(seed),node_type=int)\n",
    "    conv_3_unpruned = nx.read_graphml('conv3_{}.graphml'.format(seed),node_type=int)\n",
    "    conv_4_unpruned = nx.read_graphml('conv4_{}.graphml'.format(seed),node_type=int)\n",
    "    conv_5_unpruned = nx.read_graphml('conv5_{}.graphml'.format(seed),node_type=int)\n",
    "    # Non-iterative pruning, overwrite pruned graphs with unpruned at the start of each episode\n",
    "    nx.write_graphml_lxml(conv_2_unpruned, 'conv2.graphml')\n",
    "    nx.write_graphml_lxml(conv_3_unpruned, 'conv3.graphml')\n",
    "    nx.write_graphml_lxml(conv_4_unpruned, 'conv4.graphml')\n",
    "    nx.write_graphml_lxml(conv_5_unpruned, 'conv5.graphml')\n",
    "\n",
    "  def step(self,cfg,seed,train_loader,val_loader,x=0,y=0,measure = None):\n",
    "    if cfg['USE_PRUNED_GRAPH']:\n",
    "      %cd /content/drive/MyDrive/PRUNING/PRUNED/WS\n",
    "    # Train\n",
    "    else:\n",
    "      %cd /content/drive/MyDrive/PRUNING/UNPRUNED/WS\n",
    "    \n",
    "    model = Net(cfg,measure)\n",
    "    model.apply(weights_init)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "      print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "      model = torch.nn.DataParallel(model)\n",
    "    model.to(cfg.DEVICE)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(cfg.DEVICE)\n",
    "    optimizer = torch.optim.SGD(model.parameters(),cfg.LEARNING_RATE, cfg.MOMENTUM, cfg.WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, cfg.EPOCH)\n",
    "\n",
    "    if cfg.LOAD_TRAINED_MODEL:\n",
    "      model.load_state_dict(torch.load(cfg.TRAINED_MODEL_LOAD_DIR))\n",
    "\n",
    "    if not cfg.TEST_MODE:\n",
    "      start = time.time()\n",
    "      for epoch in range(cfg.EPOCH+1):\n",
    "        #print(\"Epoch\")\n",
    "        train(train_loader, model, criterion, optimizer, epoch, cfg)\n",
    "        scheduler.step()\n",
    "        if epoch % cfg.VAL_FREQ == 0:\n",
    "          val_loss, acc = validate(val_loader, model, criterion, cfg)\n",
    "\n",
    "          #print(\"Accuracy\",acc)\n",
    "          # maxacc=max(maxacc,acc.cpu().detach().numpy())\n",
    "          if cfg.VISDOM:\n",
    "            cfg.vis.line(X=torch.Tensor([epoch+1]).unsqueeze(0).cpu(),Y=torch.Tensor([val_loss]).unsqueeze(0).cpu(),env='torch',win=cfg.loss_window,name='val_loss',update='append')\n",
    "            cfg.vis.line(X=torch.Tensor([epoch+1]).unsqueeze(0).cpu(),Y=torch.Tensor([acc/100]).unsqueeze(0).cpu(),env='torch',win=cfg.loss_window,name='val_acc',update='append')\n",
    "      end = (time.time() - start)//60\n",
    "      #print(\"train time: {}D {}H {}M\".format(end//1440, (end%1440)//60, end%60))\n",
    "\n",
    "    lasavg,top1a=validate(val_loader, model, criterion, cfg)\n",
    "\n",
    "    prune_accuracy,prune_sensitivity,prune_specificity=performance_measure(val_loader,model,x,y,measure,cfg)\n",
    "\n",
    "    with torch.cuda.device(0):\n",
    "      flops, params = get_model_complexity_info(model, (3, 224, 224), as_strings=False, print_per_layer_stat=False)\n",
    "      global net_stats\n",
    "      net_stats['flops']=flops\n",
    "      net_stats['params']=params\n",
    "      df=pd.DataFrame(net_stats.items())\n",
    "      data=df.swapaxes(\"rows\", \"columns\")\n",
    "      labels=['conv2_nodes','conv2_edges','conv3_nodes','conv3_edges','flops','params']\n",
    "      data['Seed']=pd.Series([seed]*data.shape[0])\n",
    "      data['Graph-Model']=pd.Series(['WS']*data.shape[0])\n",
    "      data['K-value']=pd.Series([kval]*data.shape[0])\n",
    "      data['P-value']=pd.Series([pval]*data.shape[0])\n",
    "\n",
    "      data.to_csv( \"./WS_K_{}/WS_P_{}/seed_{}/{}/{}/NET_STATS_X_{}_Y_{}.csv\".format(kval,pval,num,cfg['NN']['REGIME'],measure,x,y), encoding='utf-8', sep = '\\t', header= True, index = False)\n",
    "\n",
    "    #print(\"flops : \", flops, \"    params :\", params )\n",
    "\n",
    "    return prune_accuracy,prune_sensitivity,prune_specificity, flops,params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1716633757157,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "OmMPyfHFI1FT"
   },
   "outputs": [],
   "source": [
    "def calc_unpruned_values(cfg,seed,pval,kval):\n",
    "  %cd /content/drive/MyDrive/PRUNING/UNPRUNED/WS\n",
    "  Performance  = pd.read_csv(\"./WS_K_{}/WS_P_{}/seed_{}/{}/None/PERFORMANCE_MEASURE_X_0_Y_0.csv\".format(kval,pval,num,cfg['NN']['REGIME']),sep = '\\t')\n",
    "\n",
    "\n",
    "  baseline_accuracy=float(round(Performance[Performance['Threshold']==0.1]['Accuracy'].copy(),4))\n",
    "  #print(float(round(baseline_accuracy,4)))\n",
    "  baseline_sensitivity=float(round(Performance[Performance['Threshold']==0.1]['Recall/Sensitivity'].copy(),4))\n",
    "  #print(float(round(baseline_sensitivity,4)))\n",
    "  baseline_specificity=float(round(Performance[Performance['Threshold']==0.1]['Specificity'].copy(),4))\n",
    "  #print(float(round(baseline_specificity,4)))\n",
    "\n",
    "\n",
    "  stats= pd.read_csv(\"./WS_K_{}/WS_P_{}/seed_{}/{}/None/NET_STATS_X_0_Y_0.csv\".format(kval,pval,num,cfg['NN']['REGIME'],num,pval),sep = '\\t')\n",
    "  conv2nodes=float(stats.at[1,'0'])\n",
    "  conv2edges=float(stats.at[1,'1'])\n",
    "  conv3nodes=float(stats.at[1,'2'])\n",
    "  conv3edges=float(stats.at[1,'3'])\n",
    "  flops_value=float(stats.at[1,'4'])\n",
    "  param_value=float(stats.at[1,'5'])\n",
    "  return baseline_accuracy,baseline_sensitivity,baseline_specificity,conv2nodes,conv2edges,conv3nodes,conv3edges,flops_value,param_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1716634290729,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "AgdByC4Vu2XT"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def surgery(directed_graph, threshold,edgelist):\n",
    "\n",
    "  edgelist = list(edgelist)\n",
    "  #print(\"In surgery\")\n",
    "  #print(\"Length\",len(edgelist))\n",
    "  to_remove = edgelist[:threshold]\n",
    "  #print(to_remove)\n",
    "  #if threshold==0:\n",
    "   # print(\"To Remove:\",to_remove)\n",
    "   # print(\"Number of Edges :\",directed_graph.number_of_edges())\n",
    "    #print(\"Number of nodes :\",directed_graph.number_of_nodes())\n",
    "  directed_graph.remove_edges_from(to_remove)\n",
    "   # print(\"surgery Over\")\n",
    " #  print(\"New_Length\",)\n",
    "  return directed_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1716634292778,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "LKapFFvpjiKR"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "# Function to return the list of nodes in a graph with no connected edges\n",
    "def get_isolated_nodes(directed_graph):\n",
    "  isolated_nodes = []\n",
    "  for node in list(directed_graph.nodes()):\n",
    "    if len(list(directed_graph.predecessors(node))) == 0:\n",
    "        if len(list(directed_graph.successors(node))) == 0:\n",
    "            isolated_nodes.append(node)\n",
    "  return isolated_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 791,
     "status": "ok",
     "timestamp": 1716634294414,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "lc8Ex5S9vJI-"
   },
   "outputs": [],
   "source": [
    "# Prepare a graph for feeding back into RandWire, ie. undirected remove input and output nodes\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "#def prepare_new_graph(directed_graph1, isolated_nodes):\n",
    "def prepare_new_graph(directed_graph1):\n",
    "\n",
    "  global t1\n",
    "  # Make new graph\n",
    "  #print(\"In prepare_New_Graph\")\n",
    "  directed_graph = directed_graph1\n",
    "    # Get label of highest node\n",
    "  top_node = (len(list(directed_graph.nodes()))) - 1\n",
    "  print(\"top node :\",top_node)\n",
    "  # Make sure input and output nodes will be removed even if they are not isolated\n",
    "  #isolated_nodes.append(0) if 0 not in isolated_nodes else isolated_nodes\n",
    "  #isolated_nodes.append(top_node) if top_node not in isolated_nodes else isolated_nodes\n",
    "  # print(isolated_nodes)\n",
    "  # Remove all isolated nodes\n",
    "  if directed_graph.has_node(0):\n",
    "    directed_graph.remove_node(0)\n",
    "  if directed_graph.has_node(top_node):\n",
    "    directed_graph.remove_node(top_node)\n",
    "\n",
    "  isolated_nodes=get_isolated_nodes(directed_graph)\n",
    "  #print(\"Isolated Nodes\", isolated_nodes)\n",
    "  directed_graph.remove_nodes_from(isolated_nodes)\n",
    "  new_graph = nx.Graph()\n",
    "  # Copy edges from directed graph\n",
    "  # new_edges = directed_graph.edges()\n",
    " # print(len(list(directed_graph.edges())))\n",
    "  new_edges = [(u-1, v-1) for (u,v) in list(directed_graph.edges())]\n",
    "  #print(\"num new edges : \", len(new_edges))\n",
    "  for node in list(directed_graph.nodes()):\n",
    "    if len(list(directed_graph.predecessors(node))) == 0:\n",
    "        if len(list(directed_graph.successors(node))) == 0:\n",
    "   #         print(\"PRUNED GRAPH DISCONNECTED\")\n",
    "            directed_graph.remove_nodes_from([node])\n",
    "  # print(directed_graph.nodes())\n",
    "  # nx.draw_networkx(directed_graph)\n",
    "  # Copy nodes and edges to new graph\n",
    "\n",
    "  # nx.write_graphml_lxml(directed_graph, 'conv{}_{}_pruned_fwd_withweights.graphml'.format(num,measure))\n",
    "\n",
    "  new_nodes = (list(directed_graph.nodes()))\n",
    "  new_nodes = list(np.subtract(new_nodes,1))\n",
    "\n",
    "  # Copy nodes and edges\n",
    "  new_graph.add_nodes_from(new_nodes)\n",
    "  new_graph.add_edges_from(new_edges)\n",
    "  # Make sure labels start at 0\n",
    "  new_undirected_graph = nx.convert_node_labels_to_integers(new_graph,ordering = 'sorted')\n",
    "  #print(\"Prepare_new_graph_over\")\n",
    "  return new_undirected_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 819,
     "status": "ok",
     "timestamp": 1716634298305,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "_Myoa-wcszhW"
   },
   "outputs": [],
   "source": [
    "def prune_graph(measure, seed,x,y,topcount,bottomcount,cfg):\n",
    "  %cd /content/drive/MyDrive/PRUNING/UNPRUNED/WS/\n",
    "  global kval,pval\n",
    "\n",
    "  # weights = {}\n",
    "  # else:\n",
    "  conv_2 = (nx.read_graphml('./WS_K_{}/WS_P_{}/seed_{}/{}/conv2u.graphml'.format(kval,pval,seed,cfg['NN']['REGIME']),node_type=int))\n",
    "  conv_3 = (nx.read_graphml('./WS_K_{}/WS_P_{}/seed_{}/{}/conv3u.graphml'.format(kval,pval,seed,cfg['NN']['REGIME']),node_type=int))\n",
    "  conv_4 = (nx.read_graphml('./WS_K_{}/WS_P_{}/seed_{}/{}/conv4u.graphml'.format(kval,pval,seed,cfg['NN']['REGIME']),node_type=int))\n",
    "  conv_5 = (nx.read_graphml('./WS_K_{}/WS_P_{}/seed_{}/{}/conv5u.graphml'.format(kval,pval,seed,cfg['NN']['REGIME']),node_type=int))\n",
    "\n",
    "  weights_2= nx.get_edge_attributes(conv_2,measure+\"_norm\")\n",
    "  w2 = dict(sorted(weights_2.items(), key=lambda i:(i[1],i[0]))).keys()\n",
    "  threshold2 = int(np.floor(np.percentile(np.arange(len(w2)), y)))\n",
    "  # print(\"threshold 2 :\",threshold2, \"len w2 : \",len(w2))\n",
    "  # print(w2)\n",
    "\n",
    "  weights_3= nx.get_edge_attributes(conv_3,measure+\"_norm\")\n",
    "  w3 = dict(sorted(weights_3.items(), key=lambda i:(i[1],i[0]))).keys()\n",
    "  threshold3 = int(np.floor(np.percentile(np.arange(len(w3)), x)))\n",
    "\n",
    "  weights_4= nx.get_edge_attributes(conv_4,measure+\"_norm\")\n",
    "  w4 = dict(sorted(weights_4.items(), key=lambda i:(i[1],i[0]))).keys()\n",
    "  threshold4 = int(np.floor(np.percentile(np.arange(len(w4)), x)))\n",
    "\n",
    "  weights_5= nx.get_edge_attributes(conv_5,measure+\"_norm\")\n",
    "  w5 = dict(sorted(weights_5.items(), key=lambda i:(i[1],i[0]))).keys()\n",
    "  threshold5 = int(np.floor(np.percentile(np.arange(len(w5)), x)))\n",
    "\n",
    "  conv_2_pruned = surgery(conv_2, threshold2,w2)\n",
    "  # print(\"conv 2 pruned :\",conv_2_pruned.nodes(),conv_2_pruned.edges())\n",
    "  conv_3_pruned = surgery(conv_3, threshold3,w3)\n",
    "  # print(\"conv 3 pruned :\",conv_3_pruned.nodes(),conv_3_pruned.edges())\n",
    "  conv_4_pruned = surgery(conv_4, threshold4,w4)\n",
    "  conv_5_pruned = surgery(conv_5, threshold5,w5)\n",
    "\n",
    "  new_conv_2 = prepare_new_graph(conv_2_pruned)\n",
    "  # print(\"new conv 2 : \", new_conv_2.nodes(),new_conv_2.edges())\n",
    "  new_conv_3 = prepare_new_graph(conv_3_pruned)\n",
    "  # print(\"new conv 3 : \", new_conv_3.nodes(),new_conv_3.edges())\n",
    "  new_conv_4 = prepare_new_graph(conv_4_pruned)\n",
    "  new_conv_5 = prepare_new_graph(conv_5_pruned)\n",
    "\n",
    "  %cd /content/drive/MyDrive/PRUNING/PRUNED/WS\n",
    "\n",
    "  nx.write_graphml_lxml(new_conv_2, './WS_K_{}/WS_P_{}/seed_{}/{}/output_bt/{}/conv2.graphml'.format(kval,pval,seed,cfg['NN']['REGIME'],measure))\n",
    "  nx.write_graphml_lxml(new_conv_3, './WS_K_{}/WS_P_{}/seed_{}/{}/output_bt/{}/conv3.graphml'.format(kval,pval,seed,cfg['NN']['REGIME'],measure))\n",
    "  nx.write_graphml_lxml(new_conv_4, './WS_K_{}/WS_P_{}/seed_{}/{}/output_bt/{}/conv4.graphml'.format(kval,pval,seed,cfg['NN']['REGIME'],measure))\n",
    "  nx.write_graphml_lxml(new_conv_5, './WS_K_{}/WS_P_{}/seed_{}/{}/output_bt/{}/conv5.graphml'.format(kval,pval,seed,cfg['NN']['REGIME'],measure))\n",
    "\n",
    "  nx.write_graphml_lxml(new_conv_2, './WS_K_{}/WS_P_{}/seed_{}/{}/output_bt/{}/conv2_{}_b{}_t{}.graphml'.format(kval,pval,seed,cfg['NN']['REGIME'],measure,measure,bottomcount,topcount))\n",
    "  nx.write_graphml_lxml(new_conv_3, './WS_K_{}/WS_P_{}/seed_{}/{}/output_bt/{}/conv3_{}_b{}_t{}.graphml'.format(kval,pval,seed,cfg['NN']['REGIME'],measure,measure,bottomcount,topcount))\n",
    "  nx.write_graphml_lxml(new_conv_4, './WS_K_{}/WS_P_{}/seed_{}/{}/output_bt/{}/conv4_{}_b{}_t{}.graphml'.format(kval,pval,seed,cfg['NN']['REGIME'],measure,measure,bottomcount,topcount))\n",
    "  nx.write_graphml_lxml(new_conv_5, './WS_K_{}/WS_P_{}/seed_{}/{}/output_bt/{}/conv5_{}_b{}_t{}.graphml'.format(kval,pval,seed,cfg['NN']['REGIME'],measure,measure,bottomcount,topcount))\n",
    "\n",
    "  print(len(list(new_conv_2.nodes())), len(list(new_conv_3.nodes())), len(list(new_conv_2.edges())), len(list(new_conv_3.edges())), nx.is_connected(new_conv_2), nx.is_connected(new_conv_3) )\n",
    "  return len(list(new_conv_2.nodes())), len(list(new_conv_3.nodes())), len(list(new_conv_2.edges())), len(list(new_conv_3.edges())), nx.is_connected(new_conv_2), nx.is_connected(new_conv_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1716634304245,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "aLQAmEKxiGAe"
   },
   "outputs": [],
   "source": [
    "# This function will convert the undirected graph to DAG by the following steps:\n",
    "# step 1 : assign the directions from node of lesser integer id to the node of greater integer id\n",
    "\n",
    "\n",
    "def to_dag(undirected_graph):\n",
    "\n",
    "  #changes\n",
    "\t# Initialise graph\n",
    "  directed_graph = nx.DiGraph()\n",
    "\n",
    "  for s,t in undirected_graph.edges():\n",
    "    source = min(s,t)\n",
    "    dest = max(s,t)\n",
    "    directed_graph.add_edge(source, dest)\n",
    "\n",
    "  return directed_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1716634304246,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "4c93D0YskN-K"
   },
   "outputs": [],
   "source": [
    "# This function will convert the undirected graph into DAG by,\n",
    "# step 1: adding a source node and a sink node\n",
    "# step 2: assigning direction from the node with lesser integer id to greater integer id\n",
    "# step 3: connect the nodes without predecessors to the source node(0)\n",
    "# step 4: connect the nodes without successors to the sink node(either 16 or 33 depending on if the graph passed in conv2 or conv3,4,5)\n",
    "\n",
    "def un_to_dag(undirected_graph):\n",
    "\n",
    "\n",
    "  directed_graph = nx.DiGraph()\n",
    "  for i in range(len(list(undirected_graph))):\n",
    "    neighbors = list(undirected_graph.neighbors(i))\n",
    "    for n in neighbors:\n",
    "      if  n > i:\n",
    "        directed_graph.add_edge(i+1, n+1)\n",
    "      elif i > n:\n",
    "        directed_graph.add_edge(n+1, i+1)\n",
    "          \n",
    "  for m in range(1, len(list(undirected_graph))+1):\n",
    "    n_O = len(list(directed_graph.successors(m)))\n",
    "    n_I = len(list(directed_graph.predecessors(m)))\n",
    "    if n_O == 0:\n",
    " #     print(m,\" n_O\")\n",
    "      directed_graph.add_edge(m,len(list(undirected_graph))+1)\n",
    "    if n_I == 0:\n",
    "   #   print(m,\" n_I\")\n",
    "      directed_graph.add_edge(0, m)\n",
    "    print(list(nx.selfloop_edges(directed_graph)))\n",
    "    directed_graph.remove_edges_from(nx.selfloop_edges(directed_graph))\n",
    "\n",
    "  # print(directed_graph.nodes)\n",
    "  # print(\"Directed Edges\")\n",
    "  # print(directed_graph.edges)\n",
    "  # print(\"UN-to-dag over\")\n",
    "  # print(\"*************\")\n",
    "  return directed_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1716634313768,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "L0X0j-QDiSk_",
    "outputId": "1a3d07e9-60fa-4597-9955-8d3bddb40659"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "============================================================================================\n",
    "A program to compute the Ollivier-Ricci curvature of a given directed unweighted graph.\n",
    "\n",
    "References:\n",
    "1) E. Saucan, R.P. Sreejith, R.P. Vivek-Ananth, J. Jost & A. Samal, Discrete Ricci curvatures for directed networks, Chaos, Solitons & Fractals, 118: 347-360 (2019).\n",
    "2) Ni, C.-C., Lin, Y.-Y., Gao, J., Gu, X., & Saucan, E. (2015). Ricci curvature of the Internet topology (Vol. 26, pp. 2758-2766). Presented at the 2015 IEEE Conference on Computer Communications (INFOCOM), IEEE\n",
    "\n",
    "The following is a modified version of the code that can be found in Chien-Chun Ni's github repository : https://github.com/saibalmars/GraphRicciCurvature\n",
    "\n",
    "============================================================================================\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print (\"=\"*75)\n",
    "from multiprocessing import Pool,cpu_count\n",
    "import sys\n",
    "import importlib\n",
    "import time\n",
    "import cvxpy as cvx\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# # Display date and time\n",
    "# now = datetime.datetime.now()\n",
    "# print (\"Current date and time\\n%s\\n\"%(now.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "# starttime = time.time()\n",
    "\n",
    "# Opening output files for edge and node\n",
    "# EF = open(sys.argv[2], 'w')\n",
    "# NF = open(sys.argv[3], 'w')\n",
    "\n",
    "#Creating the directed graph from the edge file\n",
    "# Graph=nx.DiGraph()\n",
    "# for i in open(sys.argv[1], 'r'):\n",
    "#   e = i.strip().split('\\t')\n",
    "#   if e[0] != e[1]:\n",
    "#     Graph.add_edge(e[0], e[1])\n",
    "\n",
    "# edgesize=Graph.number_of_edges()\n",
    "# nodesize=Graph.number_of_nodes()\n",
    "# print (\"> Created a graph with (%d) edges and (%d) nodes\"%(edgesize, nodesize))\n",
    "\n",
    "# Function for computing Olliver-Ricci curvature for a given edge.\n",
    "#============================================================================================\n",
    "\n",
    "def ricciCurvature_Edge(G, source, target, length, verbose):\n",
    "\n",
    "  # Making list of neighbours of source and target node of the edge\n",
    "  source_nbr = list(G.predecessors(source))\n",
    "  target_nbr = list(G.successors(target))\n",
    "\n",
    "  # Distributing mass to each of the neighbours of source node\n",
    "  if not source_nbr:\n",
    "    source_nbr.append(source)\n",
    "    x = [1]\n",
    "  else:\n",
    "    x=[]\n",
    "    for i in source_nbr:\n",
    "      x.append(1.0/(G.in_degree(source)+1))\n",
    "    source_nbr.append(source)\n",
    "    x.append(1.0/(G.in_degree(source)+1))\n",
    "\n",
    "  # Distributing mass to each of the neighbours of target node\n",
    "  if not target_nbr:\n",
    "    target_nbr.append(target)\n",
    "    y = [1]\n",
    "  else:\n",
    "    y=[]\n",
    "    for nbr in target_nbr:\n",
    "                        y.append(1.0/(G.out_degree(target)+1))\n",
    "    target_nbr.append(target)\n",
    "    y.append(1.0/(G.out_degree(target)+1))\n",
    "\n",
    "\n",
    "  # Construct the cost dictionary from x to y\n",
    "  d = np.zeros((len(x), len(y)))\n",
    "  for i, s in enumerate(source_nbr):\n",
    "    for j, t in enumerate(target_nbr):\n",
    "      assert t in length[s], \"Target node not in list, should not happen, pair (%d, %d)\" % (s, t)\n",
    "      # Filling the shortest path length to corresponding edges\n",
    "      d[i][j] = length[s][t]\n",
    "\n",
    "  # The mass that source neighborhood initially owned\n",
    "  x = np.array([x]).T\n",
    "  # The mass that target neighborhood needs to received\n",
    "  y = np.array([y]).T\n",
    "\n",
    "  # The transportation plan rho\n",
    "  rho = cvx.Variable(shape=(len(target_nbr), len(source_nbr)))\n",
    "\n",
    "  # objective function d(x,y) * rho * x, need to do element-wise multiply here\n",
    "  obj = cvx.Minimize(cvx.sum(cvx.multiply(np.multiply(d.T, x.T), rho)))\n",
    "\n",
    "  # \\sigma_i rho_{ij}=[1,1,...,1]\n",
    "  source_sum = cvx.sum(rho, axis=0)\n",
    "  constrains = [rho * x == y, source_sum == np.ones(len(source_nbr)), 0 <= rho, rho <= 1]\n",
    "  prob = cvx.Problem(obj, constrains)\n",
    "  m = prob.solve(solver='ECOS')\n",
    "\n",
    "  if verbose:\n",
    "    print(time.time() - t0, \" secs for cvxpy.\")\n",
    "\n",
    "  # divided by the length of d(i, j)\n",
    "  result = 1 - (m / length[source][target])\n",
    "  #print source, target, result\n",
    "  if verbose:\n",
    "    print(\"#source_nbr: %d, #target_nbr: %d, Ricci curvature = %f\" % (len(source_nbr), len(target_nbr), result))\n",
    "  return {(source, target): result}\n",
    "\n",
    "\n",
    "def _wrapRicci(stuff):\n",
    "  return ricciCurvature_Edge(*stuff)\n",
    "\n",
    "\n",
    "# Function for computing ricci curvature for all nodes and all edges in G.\n",
    "#============================================================================================\n",
    "\n",
    "def ricciCurvature(G, proc=cpu_count(), edge_list=[], verbose=False):\n",
    "  # Construct the all pair shortest path lookup\n",
    "  # t0 = time.time()\n",
    "  length = dict(nx.all_pairs_dijkstra_path_length(G, weight=None))\n",
    "  # print (\"> Time taken for all pair shortest path = %smin (%fsec)\"%(round((time.time() - t0)/60.0, 3), round((time.time() - t0), 5)))\n",
    "  # t0 = time.time()\n",
    "  # compute edge ricci curvature\n",
    "  p = Pool(processes=proc)\n",
    "\n",
    "  # if there is no assigned edges to compute, compute all edges instead\n",
    "  if not edge_list:\n",
    "    edge_list = G.edges()\n",
    "  args = [(G, source, target, length, verbose) for source, target in edge_list]\n",
    "  result = p.map_async(_wrapRicci, args)\n",
    "  result = result.get()\n",
    "  p.close()\n",
    "  p.join()\n",
    "\n",
    "  # assign edge Ricci curvature from result to graph G\n",
    "  for rc in result:\n",
    "    for k in list(rc.keys()):\n",
    "      source, target = k\n",
    "      G[source][target]['ORC'] = rc[k]\n",
    "  # endtime = time.time()\n",
    "  # print (\"> Time taken for OR Curvature of edge = %smin (%fsec)\"%(round((time.time() - t0)/60.0, 3), round((time.time() - t0), 5)))\n",
    "\n",
    "  # compute node Ricci curvature\n",
    "  print (\"\\nOlliver-Ricci curvature for node\")\n",
    "\n",
    "  for n in G.nodes():\n",
    "    in_rcsum = 0  # sum of the incomming neighbor Ricci curvature\n",
    "    if G.in_degree(n) != 0:\n",
    "      for nbr in G.predecessors(n):\n",
    "        if 'ORC' in G[nbr][n]:\n",
    "          in_rcsum += G[nbr][n]['ORC']\n",
    "\n",
    "    out_rcsum = 0  # sum of the outgoing neighbor Ricci curvature\n",
    "    if G.out_degree(n) != 0:\n",
    "      for nbr in G.successors(n):\n",
    "        if 'ORC' in G[n][nbr]:\n",
    "          out_rcsum += G[n][nbr]['ORC']\n",
    "\n",
    "    # NF.write(\"%s\\t%s\\t%s\\n\"%(n, in_rcsum, out_rcsum))\n",
    "\n",
    "  # endtime = time.time()\n",
    "  # print (\"> Time taken for OR Curvature of node = %smin (%fsec)\"%(round((time.time() - t0)/60.0, 3), round((time.time() - t0), 5)))\n",
    "  return G\n",
    "\n",
    "#============================================================================================\n",
    "# Calling the main function to compute Olliver-Ricci curvature of the graph\n",
    "\n",
    "# print (\"\\nComputation for Olliver-Ricci curvature for edge started\")\n",
    "# Elist=Graph.edges()# Edge list of the graph\n",
    "# ricciCurvature(Graph, proc=10, edge_list=[], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1716634351760,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "UWhfMkiAiTYf"
   },
   "outputs": [],
   "source": [
    "def FR_Dir(G):\n",
    "  for i in G.edges():\n",
    "    u=i[0]\n",
    "    v=i[1]\n",
    "    fc=0\n",
    "    fc+=2\n",
    "    in_u=list(G.in_edges(u))\n",
    "    fc-=len(in_u)\n",
    "    out_v=list(G.out_edges(v))\n",
    "    fc-=len(out_v)\n",
    "    G[u][v]['FR']=fc\n",
    "  return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1716634356157,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "TEge0BSEhWlU"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "def calc_edge_measures(G):\n",
    "\n",
    "  G = to_dag(G)\n",
    "  G = ricciCurvature(G)\n",
    "  G = FR_Dir(G)\n",
    "  ebc = nx.edge_betweenness_centrality(G)\n",
    "  for (u,v), value in ebc.items() : G[u][v]['EBC'] = value\n",
    "\n",
    "  return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1716634356767,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "pqgoEl-ChdfI"
   },
   "outputs": [],
   "source": [
    "def normalize_weights(G):\n",
    "\n",
    "  for measure in ['FR','AFR','ORC','EBC']:\n",
    "    weights = list(nx.get_edge_attributes(G,measure).values())\n",
    "    # print(\"measure\",measure,\" \",weights)\n",
    "    minval = np.min(weights)\n",
    "    maxval = np.max(weights)\n",
    "    # if measure==\"EBC\":\n",
    "      # print(\"MIN\",minval)\n",
    "      # print(\"MAX\",maxval)\n",
    "    if (measure  == 'EBC'):\n",
    "      for (u,v) in G.edges():\n",
    "        x = G[u][v][measure]\n",
    "        #print(\"X\",x)\n",
    "        G[u][v][measure+\"_norm\"] = ((x-minval+1)/(maxval - minval+2))\n",
    "        #print(\"X\",x,\" \",G[u][v][measure+\"_norm\"] )\n",
    "    else:\n",
    "      for (u,v) in G.edges():\n",
    "        x = G[u][v][measure]\n",
    "        G[u][v][measure+\"_norm\"] = ((maxval-x+1)/(maxval-minval+2))\n",
    "  return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1716634358831,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "9DESUaPqf473"
   },
   "outputs": [],
   "source": [
    "def normalized_edge_measures_dir(cfg,seed):\n",
    "\n",
    "  %cd /content/drive/MyDrive/PRUNING/UNPRUNED/WS/\n",
    "  global kval,pval\n",
    "\n",
    "  conv_2 = un_to_dag(nx.read_graphml('./WS_K_{}/WS_P_{}/seed_{}/{}/conv2_{}.graphml'.format(kval,pval,seed,cfg['NN']['REGIME'],cfg['RND_SEED']),node_type=int))\n",
    "  conv_3 = un_to_dag(nx.read_graphml('./WS_K_{}/WS_P_{}/seed_{}/{}/conv3_{}.graphml'.format(kval,pval,seed,cfg['NN']['REGIME'],cfg['RND_SEED']),node_type=int))\n",
    "  conv_4 = un_to_dag(nx.read_graphml('./WS_K_{}/WS_P_{}/seed_{}/{}/conv4_{}.graphml'.format(kval,pval,seed,cfg['NN']['REGIME'],cfg['RND_SEED']),node_type=int))\n",
    "  conv_5 = un_to_dag(nx.read_graphml('./WS_K_{}/WS_P_{}/seed_{}/{}/conv5_{}.graphml'.format(kval,pval,seed,cfg['NN']['REGIME'],cfg['RND_SEED']),node_type=int))\n",
    "\n",
    "\n",
    "  conv_2 = calc_edge_measures(conv_2).copy()\n",
    "  conv_3=  calc_edge_measures(conv_3).copy()\n",
    "  conv_4 = calc_edge_measures(conv_4).copy()\n",
    "  conv_5 = calc_edge_measures(conv_5).copy()\n",
    "\n",
    "  conv_2_ = normalize_weights(conv_2)\n",
    "  conv_3_ = normalize_weights(conv_3)\n",
    "  conv_4_ = normalize_weights(conv_4)\n",
    "  conv_5_ = normalize_weights(conv_5)\n",
    "\n",
    "\n",
    "  nx.write_graphml(conv_2_, \"./WS_K_{}/WS_P_{}/seed_{}/{}/conv2u.graphml\".format(kval,pval,seed,cfg['NN']['REGIME']))\n",
    "  nx.write_graphml(conv_3_, \"./WS_K_{}/WS_P_{}/seed_{}/{}/conv3u.graphml\".format(kval,pval,seed,cfg['NN']['REGIME']))\n",
    "  nx.write_graphml(conv_4_, \"./WS_K_{}/WS_P_{}/seed_{}/{}/conv4u.graphml\".format(kval,pval,seed,cfg['NN']['REGIME']))\n",
    "  nx.write_graphml(conv_5_, \"./WS_K_{}/WS_P_{}/seed_{}/{}/conv5u.graphml\".format(kval,pval,seed,cfg['NN']['REGIME']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1716634361728,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "rc5xGP0VIeIG"
   },
   "outputs": [],
   "source": [
    "def Edgeparams_BT(cfg, seed,train_loader,val_loader):\n",
    "\n",
    "  %cd /content/drive/MyDrive/PRUNING/PRUNED/WS\n",
    "\n",
    "  global measures,pval,kval\n",
    "\n",
    "  baseline_acc,baseline_sensitivity,baseline_specificity,conv2nodes,conv2edges,conv3nodes,conv3edges,flops_value,param_value = calc_unpruned_values(cfg,seed,pval,kval)\n",
    "  #baseline_acc = 0.8085\n",
    "  normalized_edge_measures_dir(cfg, seed)\n",
    "  # prune_graph(\"FR\",14, 50,1)\n",
    "  df1 = pd.DataFrame(columns=[\"randseed\",\"measure\",\"bottom_depth\",'top_depth', \"xvalue\",\"yvalue\", \"nodes2\",\"edges2\",\"nodes3\",\"edges3\",\"act_nodes_2\",\"act_edges2\",\"act_nodes_3\",\"act_edges3\",\"connectivity_conv2\",\"connectivity_conv3\",\"pruning_accuracy\",\"baseline_acc\",\"pruning_sensitivity\",\"baseline_sensitivity\",\"pruning_specificity\",\"baseline_specificity\",\"pruning_flops\",\"baseline_flops\",\"pruning_parameters\",\"baseline_parameters\",\"%edges\",\"%flops\",\"%parameters\"])\n",
    "  i = 0\n",
    "  %cd /content/drive/MyDrive/PRUNING/PRUNED/WS\n",
    "  for measure in measures:\n",
    "    if (not os.path.isdir(\"./WS_K_{}/WS_P_{}/seed_{}/{}/output_bt/{}\".format(kval,pval,num,cfg['NN']['REGIME'],measure))):\n",
    "        os.makedirs(\"./WS_K_{}/WS_P_{}/seed_{}/{}/output_bt/{}\".format(kval,pval,num,cfg['NN']['REGIME'],measure))\n",
    "    print(\"MEASURE : \",measure)\n",
    "    #count = 1\n",
    "\n",
    "    top_count = 1\n",
    "    best_acc = baseline_acc\n",
    "     #To run from the last breaking point\n",
    "    if os.path.exists(\"./WS_K_{}/WS_P_{}/seed_{}/{}/output_bt/{}_torun_x.csv\".format(kval,pval,num,cfg['NN']['REGIME'],measure)):\n",
    "      with open(\"./WS_K_{}/WS_P_{}/seed_{}/{}/output_bt/{}_torun_x.csv\".format(kval,pval,num,cfg['NN']['REGIME'],measure)) as f:\n",
    "        lis = [line.split() for line in f]\n",
    "        print(lis)\n",
    "        xmin=float(lis[0][0])\n",
    "        xmax=float(lis[1][0])\n",
    "        x=float(lis[2][0])\n",
    "        best_x=float(lis[3][0])\n",
    "        bottom_count=int(lis[4][0])\n",
    "        i=int(lis[5][0])\n",
    "        f.close()\n",
    "    else:\n",
    "      xmin = 0\n",
    "      xmax = 100\n",
    "      x = (xmin + xmax)/2\n",
    "      # prev_acc= baseline_acc\n",
    "      best_x = 0\n",
    "      bottom_count = 1\n",
    "\n",
    "    if os.path.exists(\"./WS_K_{}/WS_P_{}/seed_{}/{}/output_bt/{}_stats.csv\".format(kval,pval,num,cfg['NN']['REGIME'],measure)):\n",
    "      df1=pd.read_csv(\"./WS_K_{}/WS_P_{}/seed_{}/{}/output_bt/{}_stats.csv\".format(kval,pval,num,cfg['NN']['REGIME'],measure))\n",
    "      print(\"Inside if\")\n",
    "      print(df1)\n",
    "\n",
    "    best_x = 0\n",
    "\n",
    "    y = 0\n",
    "    while (bottom_count <= 5):\n",
    "      print(\"measure: \",measure, \" topcount : \", top_count, \" bottomcount : \", bottom_count,\" ; x value : \", x)\n",
    "      nodes2, nodes3, edges2, edges3, connectivity_conv2, connectivity_conv3 = prune_graph(measure,seed, x,y,top_count,bottom_count,cfg)\n",
    "\n",
    "      top1s,sensitivity,specificity, flops,param = env.step(cfg,seed,train_loader,val_loader ,x,y, measure)\n",
    "\n",
    "      if round(top1s,4) >= baseline_acc and round(sensitivity,4)>=baseline_sensitivity and round(specificity,4)>=baseline_specificity:\n",
    "        if (x > best_x):\n",
    "            best_x = x\n",
    "      print(\"best_x : \", best_x)\n",
    "\n",
    "      df1 = df1.append(pd.DataFrame([[seed,measure,bottom_count,top_count,x,y,nodes2,edges2,nodes3,edges3,conv2nodes,conv2edges,conv3nodes, conv3edges,connectivity_conv2,connectivity_conv3,top1s,baseline_acc,sensitivity,baseline_sensitivity,specificity,baseline_specificity,flops_value,flops,param,param_value,((edges2+(3*edges3))/(conv2edges + (3*conv3edges)))*100, (flops/flops_value)*100,(param/param_value)*100]],columns=list(df1.columns)),ignore_index=True)\n",
    "      if (bottom_count < 5):\n",
    "          \n",
    "        if round(top1s,4) >= baseline_acc and round(sensitivity,4)>=baseline_sensitivity and round(specificity,4)>=baseline_specificity:\n",
    "          xmin = x\n",
    "          xmax = xmax\n",
    "          x = (xmin + xmax)/2\n",
    "        else:\n",
    "          xmin = xmin\n",
    "          xmax = x\n",
    "          x = (xmin + xmax)/2\n",
    "\n",
    "      bottom_count += 1\n",
    "      i += 1\n",
    "      #writing the last updated value\n",
    "      nex_run=[[xmin],[xmax],[x],[best_x],[bottom_count],[i]]\n",
    "      nex_file = open(\"./WS_K_{}/WS_P_{}/seed_{}/{}/output_bt/{}_torun_x.csv\".format(kval,pval,num,cfg['NN']['REGIME'],measure), 'w', newline ='')\n",
    "      with nex_file:\n",
    "        write = csv.writer(nex_file)\n",
    "        write.writerows(nex_run)\n",
    "      nex_file.close()\n",
    "      df1.to_csv(\"./WS_K_{}/WS_P_{}/seed_{}/{}/output_bt/{}_stats.csv\".format(kval,pval,seed,cfg['NN']['REGIME'],measure), header = True, index = False)\n",
    "    best_val=np.array([best_x,y])\n",
    "\n",
    "    Best_DF = pd.DataFrame(best_val)\n",
    "    Best_DF.to_csv(\"./WS_K_{}/WS_P_{}/seed_{}/{}/output_bt/{}_Best.csv\".format(kval,pval,seed,cfg['NN']['REGIME'],measure), header = True, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2537,
     "status": "ok",
     "timestamp": 1716634370099,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "ZoWQKAYdVKUo"
   },
   "outputs": [],
   "source": [
    "train_loader=torch.load('/content/drive/MyDrive/Outputs/train_data.pth')\n",
    "val_loader=torch.load('/content/drive/MyDrive/Outputs/val_data.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "%cd /content/drive/MyDrive/\n",
    "randseeds = [3, 16, 34, 57, 59, 61, 66, 72, 92, 97]\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Start  Time =\", current_time)\n",
    "net_stats={}\n",
    "env = set_environment()\n",
    "cfg = get_configuration()\n",
    "%cd /content/drive/MyDrive/PRUNING/UNPRUNED/WS\n",
    "cfg[\"USE_PRUNED_GRAPH\"] = False\n",
    "cfg[\"MAKE_GRAPH\"] = True\n",
    "cfg[\"EPOCH\"] = 100 \n",
    "cfg[\"GRAPH_MODEL\"] = \"WS\"  # 'ER', 'BA'\n",
    "cfg[\"WS_K\"] = 4\n",
    "cfg[\"WS_P\"]=0.75\n",
    "pval = cfg['WS_P']\n",
    "kval = cfg['WS_K']\n",
    "cfg['NN']['REGIME'] = 'SMALL'\n",
    "prepare(cfg)\n",
    "seed = 0\n",
    "# count=2\n",
    "if (cfg['MAKE_GRAPH']):\n",
    "  global seed\n",
    "  for num in randseeds:\n",
    "    #print(\"Seed :\",num)\n",
    "    global net_stats\n",
    "    net_stats={}\n",
    "    cfg['RND_SEED'] = num\n",
    "    seed = num\n",
    "\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive/PRUNING/UNPRUNED/WS/WS_K_{}/WS_P_{}/\".format(kval,pval)):\n",
    "      os.makedirs(\"/content/drive/MyDrive/PRUNING/UNPRUNED/WS/WS_K_{}/WS_P_{}/\".format(kval,pval))\n",
    "\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive/PRUNING/UNPRUNED/WS/WS_K_{}/WS_P_{}/seed_{}\".format(kval,pval,num)):\n",
    "      os.makedirs(\"/content/drive/MyDrive/PRUNING/UNPRUNED/WS/WS_K_{}/WS_P_{}/seed_{}\".format(kval,pval,num))\n",
    "\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive/PRUNING/UNPRUNED/WS/WS_K_{}/WS_P_{}/seed_{}/{}\".format(kval,pval,num,cfg['NN']['REGIME'])):\n",
    "      os.makedirs(\"/content/drive/MyDrive/PRUNING/UNPRUNED/WS/WS_K_{}/WS_P_{}/seed_{}/{}\".format(kval,pval,num,cfg['NN']['REGIME']))\n",
    "\n",
    "    top_1s,sensetivity,specificity, flops, params = env.step(cfg,num,train_loader,val_loader)\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"End Time =\", current_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "error",
     "timestamp": 1716635043056,
     "user": {
      "displayName": "Areejit Samal",
      "userId": "03106480637751642097"
     },
     "user_tz": -330
    },
    "id": "SAiIe1GA_ajo",
    "outputId": "39d6a2f4-d99e-4dad-f618-c0741d482b36"
   },
   "outputs": [],
   "source": [
    "## Pruning\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "%cd /content/drive/MyDrive/\n",
    "randseeds = [3, 16, 34, 57, 59, 61, 66, 72, 92, 97]\n",
    "measures= ['FR'] #'ORC', 'EBC'\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Start  Time =\", current_time)\n",
    "net_stats={}\n",
    "env = set_environment()\n",
    "cfg = get_configuration()\n",
    "%cd /content/drive/MyDrive/PRUNING/PRUNED/WS\n",
    "cfg[\"USE_PRUNED_GRAPH\"] = True\n",
    "cfg[\"MAKE_GRAPH\"] = False\n",
    "cfg[\"EPOCH\"] = 2\n",
    "cfg[\"GRAPH_MODEL\"] = \"WS\"\n",
    "cfg[\"WS_K\"] = 4\n",
    "cfg[\"WS_P\"]=0.75\n",
    "pval = cfg['WS_P']\n",
    "kval = cfg['WS_K']\n",
    "cfg['NN']['REGIME'] = 'SMALL'\n",
    "prepare(cfg)\n",
    "seed = 0\n",
    "# count=2\n",
    "\n",
    "if not cfg[\"MAKE_GRAPH\"]:\n",
    "  for num in randseeds:\n",
    "    seed=num\n",
    "    print(\"RANDOM SEED : \", num)\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive/PRUNING/PRUNED/WS/WS_K_{}/\".format(cfg['WS_K'])):\n",
    "      os.makedirs(\"/content/drive/MyDrive/PRUNING/PRUNED/WS/WS_K_{}/\".format(cfg['WS_K']))\n",
    "\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive/PRUNING/PRUNED/WS/WS_K_{}/WS_P_{}/\".format(kval,pval)):\n",
    "      os.makedirs(\"/content/drive/MyDrive/PRUNING/PRUNED/WS/WS_K_{}/WS_P_{}/\".format(kval,pval))\n",
    "\n",
    "    if (not os.path.isdir(\"./WS_K_{}/WS_P_{}/seed_{}\".format(kval,pval,num))):\n",
    "        os.makedirs(\"./WS_K_{}/WS_P_{}/seed_{}\".format(kval,pval,num))\n",
    "\n",
    "    if (not os.path.isdir(\"./WS_K_{}/WS_P_{}/seed_{}/{}\".format(kval,pval,num,cfg['NN']['REGIME']))):\n",
    "        os.makedirs(\"./WS_K_{}/WS_P_{}/seed_{}/{}\".format(kval,pval,num,cfg['NN']['REGIME']))\n",
    "\n",
    "    if (not os.path.isdir(\"./WS_K_{}/WS_P_{}/seed_{}/{}/output_bt/\".format(kval,pval,num,cfg['NN']['REGIME']))):\n",
    "        os.makedirs(\"./WS_K_{}/WS_P_{}/seed_{}/{}/output_bt/\".format(kval,pval,num,cfg['NN']['REGIME']))\n",
    "\n",
    "    cfg['RND_SEED'] = num\n",
    "    Edgeparams_BT(cfg, num,train_loader,val_loader)\n",
    "    print(\"Random seed: \",num,\" over\")\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"End Time =\", current_time)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
